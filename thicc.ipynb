{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54d8d7f",
   "metadata": {},
   "source": [
    "# THICC: Tennessee's Hottest Industries by County Calculator\n",
    "\n",
    "This notebook utilizes the Agency for Healthcare Research and Quality (AHRQ)'s Social Determinants of Health (SDOH) Database to train a Random Forest regression model to predict the best Tennessee counties for employment given a user's job industry, education level, and other socioeconomic variables. The user will see the top 5 best Tennessee counties and the overall employment landscape in Tennessee for a given industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36db2ff",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac204c",
   "metadata": {},
   "source": [
    "This project utilizes AHRQ SDOH data and the U.S. Census Bureau's census tract data. Due to the large file sizes, all spreadsheets, CSVs, and Shapefiles can be found on Harvard Dataverse (https://doi.org/10.7910/DVN/GIOAZY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0996028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import collections\n",
    "\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa60925",
   "metadata": {},
   "source": [
    "### Load in AHRQ SDOH data\n",
    "\n",
    "The original AHRQ SDOH data comes in Excel spreadsheet format. The spreadsheet contains two sheets: LAYOUT and DATA. The LAYOUT sheet contains the column variable names and written descriptions. The DATA sheet contains the data and statistics for those column variable names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c36deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/sdoh_2020_tract_1_0.xlsx'\n",
    "dfs = pd.read_excel(\n",
    "        path,\n",
    "        sheet_name = None,\n",
    "        dtype = str,\n",
    "        na_filter = False,\n",
    "        nrows = 1000,\n",
    ")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(name)\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        display(df.head())\n",
    "\n",
    "layout_df = dfs['Layout']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b7ac0",
   "metadata": {},
   "source": [
    "Let's look at the LAYOUT sheet and identify the data types of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope():\n",
    "    nonnumerics = []\n",
    "    for name, row in layout_df.iterrows():\n",
    "        if row['type'] != 'num':\n",
    "            nonnumerics.append(name)\n",
    "\n",
    "    for name in nonnumerics:\n",
    "        print(f'        {name!r}: str,')\n",
    "\n",
    "scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d731e",
   "metadata": {},
   "source": [
    "Now we know to identify some of the columns as strings instead of floats. Reading the entire DATA sheet as an Excel spreadsheet takes on average 3 minutes. To speed up the process, let's use the CSV version of the DATA sheet instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a414b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/sdoh_2020_tract_1_0_data.csv'\n",
    "data_df = pd.read_csv(\n",
    "    path,\n",
    "    dtype = collections.defaultdict(lambda: float) | {\n",
    "        'TRACTFIPS': str,\n",
    "        'COUNTYFIPS': str,\n",
    "        'STATEFIPS': str,\n",
    "        'STATE': str,\n",
    "        'COUNTY': str,\n",
    "        'REGION': str,\n",
    "        'CEN_AIAN_NH_IND': str,\n",
    "    },\n",
    "    na_filter = True,\n",
    "    na_values=['', ' '],\n",
    ")\n",
    "display(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c24f1",
   "metadata": {},
   "source": [
    "Let's print out some information on the data in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display info on the data columns\n",
    "data_df.info()\n",
    "\n",
    "#Generate statistics on the data columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88263ac7",
   "metadata": {},
   "source": [
    "### Merge AHRQ SDOH DataFrame with TN Census Tract shapefile\n",
    "The U.S. Census Bureau has a Census Tract shapefile for Tennessee in 2020, so we can give the Tennessee counties a geometry to plot to.\n",
    "Let's merge our AHRQ DataFrame with the Census Tract GeoPandas GeoDataFrame (GDF) by the matching TRACTFIPS and GEOID respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile = gpd.read_file('data/tl_2020_47_tract/tl_2020_47_tract.shp')\n",
    "display(shapefile.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After performing the merge\n",
    "merged = data_df.merge(shapefile[['GEOID', 'geometry']], left_on='TRACTFIPS', right_on='GEOID', how='left')\n",
    "\n",
    "#Reorder the columns to move 'GEOID' and 'geometry' to the front\n",
    "columns = ['GEOID', 'geometry'] + [col for col in merged.columns if col not in ['GEOID', 'geometry']]\n",
    "merged = merged[columns]\n",
    "\n",
    "gdf = gpd.GeoDataFrame(merged, geometry='geometry')\n",
    "tennessee_gdf = gdf[gdf[\"GEOID\"].astype(str).str.startswith('47')]\n",
    "display(tennessee_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83379227",
   "metadata": {},
   "source": [
    "Now that the GDF has all the information, let's filter it down to just the columns we need. In particular, we keep columns with county/geographic information, median age, average household income, percentage of population education, and percentage of population with specific industry jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tennessee_gdf = tennessee_gdf[\n",
    "    [\n",
    "        'GEOID',\n",
    "        'geometry',\n",
    "        'STATE',\n",
    "        'COUNTY',\n",
    "        'COUNTYFIPS',\n",
    "        'CEN_POPDENSITY_TRACT',\n",
    "        'ACS_TOT_CIVILIAN_LABOR',\n",
    "        'ACS_TOT_CIVIL_EMPLOY_POP',\n",
    "\n",
    "        'ACS_MEDIAN_AGE',\n",
    "\n",
    "\n",
    "        'ACS_PCT_LT_HS',\n",
    "        'ACS_PCT_HS_GRADUATE',\n",
    "        'ACS_PCT_COLLEGE_ASSOCIATE_DGR',   \n",
    "        'ACS_PCT_BACHELOR_DGR',\n",
    "        'ACS_PCT_GRADUATE_DGR',\n",
    "        'ACS_PCT_POSTHS_ED',\n",
    "        \n",
    "        'ACS_MEDIAN_HH_INC',\n",
    "        'ACS_MEDIAN_INC_F',\n",
    "        'ACS_MEDIAN_INC_M',\n",
    "        \n",
    "        'ACS_PCT_EMPLOYED',\n",
    "        'ACS_PCT_NOT_LABOR',\n",
    "        'ACS_PCT_UNEMPLOY',\n",
    "\n",
    "        'ACS_PCT_ADMIN',\n",
    "        'ACS_PCT_ARMED_FORCES',\n",
    "        'ACS_PCT_ART',\n",
    "        'ACS_PCT_CONSTRUCT',\n",
    "        'ACS_PCT_EDUC',\n",
    "        'ACS_PCT_FINANCE',\n",
    "        'ACS_PCT_GOVT',\n",
    "        'ACS_PCT_INFORM',\n",
    "        'ACS_PCT_MANUFACT',\n",
    "        'ACS_PCT_NATURE',\n",
    "        'ACS_PCT_OTHER',\n",
    "        'ACS_PCT_PROFESS',\n",
    "        'ACS_PCT_RETAIL',\n",
    "        'ACS_PCT_TRANSPORT',\n",
    "        'ACS_PCT_WHOLESALE',\n",
    "    ]\n",
    "]\n",
    "\n",
    "display(filtered_tennessee_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc60908",
   "metadata": {},
   "source": [
    "Now that we filtered our GDF, let's look at what kind of data we're handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tennessee_gdf.info()\n",
    "\n",
    "filtered_tennessee_gdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4a03b",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "Let's remove empty values and preprocess the data. First, let's check how many empty values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70376050",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shape = filtered_tennessee_gdf.shape\n",
    "print(\"GDF shape: \", old_shape)\n",
    "\n",
    "highest_empty_count = 0\n",
    "for i in filtered_tennessee_gdf:\n",
    "    if filtered_tennessee_gdf[i].isna().sum() != 0:\n",
    "        print(f\"{i}: {filtered_tennessee_gdf[i].isna().sum()}\")\n",
    "    if filtered_tennessee_gdf[i].isna().sum() > highest_empty_count:\n",
    "        highest_empty_count = filtered_tennessee_gdf[i].isna().sum()\n",
    "\n",
    "print(highest_empty_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ee204",
   "metadata": {},
   "source": [
    "Out of 1700 rows of data, dropping ~19 rows will not be a lot of lost data. Let's drop the empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cace4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_filtered_tennessee_gdf = filtered_tennessee_gdf.dropna()\n",
    "deleted_rows = old_shape[0] - clean_filtered_tennessee_gdf.shape[0]\n",
    "print(f\"Old shape: {old_shape}\")\n",
    "print(f\"New shape: {clean_filtered_tennessee_gdf.shape}\")\n",
    "print(f\"Deleted rows: {deleted_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfbff9",
   "metadata": {},
   "source": [
    "## Create Random Forest model\n",
    "Now that our data is ready to be used, let's create a predictor model. A random forest is an ensemble method that creates multiple decision trees by bootstrapping data samples and using random subsets of features for each tree. The final prediction is made by averaging the outputs of all trees, reducing overfitting and improving accuracy compared to a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9992db",
   "metadata": {},
   "source": [
    "i should explain here why random forest is a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a659fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ac245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_filtered_tennessee_gdf.copy()\n",
    "\n",
    "# Drop geometry column for ML training\n",
    "df_ml = df.drop(columns=['geometry'])\n",
    "\n",
    "# Create dictionary mappings for career fields to relevant features\n",
    "career_field_features = {\n",
    "    'armed forces' : 'ACS_PCT_ARMED_FORCES',\n",
    "    'arts (entertainment, recreation, accommodation, and food services)' : 'ACS_PCT_ART',\n",
    "    'construction' : 'ACS_PCT_CONSTRUCT',\n",
    "    'educational services (healthcare, social assistance)' : 'ACS_PCT_EDUC',\n",
    "    'finance (insurance, real estate, rental/leasing)' : 'ACS_PCT_FINANCE',\n",
    "    'government' : 'ACS_PCT_GOVT',\n",
    "    'informaton services' : 'ACS_PCT_INFORM',\n",
    "    'manufacturing' : 'ACS_PCT_MANUFACT',\n",
    "    'nature (agriculature, forestry, fishing, hunting, mining)' : 'ACS_PCT_NATURE',\n",
    "    'other': 'ACS_PCT_OTHER',\n",
    "    'professional (scientific, management, administrative, and waste management)' : 'ACS_PCT_PROFESS',\n",
    "    'public administration' : 'ACS_PCT_ADMIN',\n",
    "    'retail' : 'ACS_PCT_RETAIL',\n",
    "    'transportation (warehousing, utilities)' : 'ACS_PCT_TRANSPORT',\n",
    "    'wholesale' : 'ACS_PCT_WHOLESALE',\n",
    "}\n",
    "\n",
    "# Create education level features\n",
    "education_levels = {\n",
    "    'less_than_high_school': 'ACS_PCT_LT_HS',\n",
    "    'high_school': 'ACS_PCT_HS_GRADUATE',\n",
    "    'some_college/associates': 'ACS_PCT_COLLEGE_ASSOCIATE_DGR',\n",
    "    'bachelors': 'ACS_PCT_BACHELOR_DGR',\n",
    "    'masters/doctorate': 'ACS_PCT_GRADUATE_DGR',   \n",
    "    'postsecondary': 'ACS_PCT_POSTHS_ED'\n",
    "}\n",
    "\n",
    "\n",
    "feature_names = [\n",
    "    'ACS_MEDIAN_HH_INC', \n",
    "    'ACS_MEDIAN_INC_F',\n",
    "    'ACS_MEDIAN_INC_M',\n",
    "\n",
    "    'ACS_TOT_CIVILIAN_LABOR',\n",
    "    'ACS_TOT_CIVIL_EMPLOY_POP',\n",
    "\n",
    "    'ACS_PCT_LT_HS',\n",
    "    'ACS_PCT_HS_GRADUATE',\n",
    "    'ACS_PCT_COLLEGE_ASSOCIATE_DGR',\n",
    "    'ACS_PCT_BACHELOR_DGR',\n",
    "    'ACS_PCT_GRADUATE_DGR',\n",
    "    'ACS_PCT_POSTHS_ED',\n",
    "\n",
    "    'ACS_PCT_ARMED_FORCES',\n",
    "    'ACS_PCT_ART',\n",
    "    'ACS_PCT_CONSTRUCT',\n",
    "    'ACS_PCT_EDUC',\n",
    "    'ACS_PCT_FINANCE',\n",
    "    'ACS_PCT_GOVT',\n",
    "    'ACS_PCT_INFORM',\n",
    "    'ACS_PCT_MANUFACT',\n",
    "    'ACS_PCT_NATURE',\n",
    "    'ACS_PCT_OTHER',\n",
    "    'ACS_PCT_PROFESS',\n",
    "    'ACS_PCT_ADMIN',\n",
    "    'ACS_PCT_RETAIL',\n",
    "    'ACS_PCT_TRANSPORT',\n",
    "    'ACS_PCT_WHOLESALE'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c405e",
   "metadata": {},
   "source": [
    "### Train the RF model\n",
    "Let's create a function to train a random forest model to predict industry suitability scores for counties. Creating a score that factors in household income, population density, and other variables will make it easier for the regression model to predict and return something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_career_hotspot_rf_model():\n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "    df = clean_filtered_tennessee_gdf.copy()\n",
    "\n",
    "    scaler = MinMaxScaler() \n",
    "    df.loc[:, feature_names] = scaler.fit_transform(df[feature_names])\n",
    "\n",
    "\n",
    "    # Loop through each county\n",
    "    for _, county in df.iterrows():\n",
    "        features = [county.get(f, 0) for f in feature_names]\n",
    "        feature_vectors.append(features)\n",
    "        \n",
    "        # Create a composite target that represents overall career suitability\n",
    "        target_score = 0        \n",
    "        for f in feature_names:\n",
    "            target_score += county.get(f, 0)\n",
    "        \n",
    "        #for career, column in career_field_features.items():\n",
    "        #    target_score += county.get(column, 0) \n",
    "        \n",
    "        \n",
    "        labels.append(target_score)\n",
    "\n",
    "\n",
    "    #Split the data into training 70% and testing 30%\n",
    "    X = np.array(feature_vectors)\n",
    "    y = np.array(labels)\n",
    "    x_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    \n",
    "    #Train model with cross-validation to avoid overfitting\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=10,  # Prevent overfitting\n",
    "        min_samples_split=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    #Check model performance with cross-validation\n",
    "    cv_scores = cross_val_score(model, x_train_reg, y_train_reg, cv=5, scoring='r2')\n",
    "    print(f\"Cross-validation R² scores: {cv_scores}\")\n",
    "    print(f\"Mean R² score: {cv_scores.mean():.3f}\")\n",
    "    \n",
    "    #Do the real training/testing split\n",
    "    model.fit(x_train_reg, y_train_reg)\n",
    "    y_pred_reg = model.predict(x_test_reg)\n",
    "    mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "    mape = mean_absolute_percentage_error(y_test_reg, y_pred_reg)\n",
    "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "    print(\"Mean Average Error (MAE) score: \", mae)\n",
    "    print(\"Mean Squared Error (MSE) score: \", mse)\n",
    "    print(\"Mean Absolute Percentage Error (MAPE) score : \", mape)\n",
    "    print(\"R² score: \", r2)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return model, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4158288",
   "metadata": {},
   "source": [
    "### Predict with RF model\n",
    "Now that we trained our RF model, let's predict industry suitability scores for all Tennessee counties given user-inputted industry name and education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c83d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model_for_industry(model, feature_names, selected_career, selected_education_level):\n",
    "    # Prepare feature matrix\n",
    "    features_matrix = clean_filtered_tennessee_gdf[feature_names].fillna(0).values\n",
    "\n",
    "    # Base predictions from the model (this gives the general career suitability score)\n",
    "    base_preds = model.predict(features_matrix)\n",
    "\n",
    "    # Create prediction DataFrame\n",
    "    prediction_df = clean_filtered_tennessee_gdf[['GEOID', 'COUNTY', 'COUNTYFIPS', selected_career, selected_education_level]].copy()\n",
    "    prediction_df['predicted_score'] = base_preds\n",
    "\n",
    "\n",
    "    '''\n",
    "    This approach ensures that counties with particularly strong metrics for your selected career and education level get ranked higher than they might in the general prediction.\n",
    "    For example, if you're interested in healthcare careers for people with bachelor's degrees:\n",
    "\n",
    "    Some counties might score well overall but not have strong healthcare employment\n",
    "    Others might have excellent healthcare employment but lower scores in other areas\n",
    "    Your boosting would help identify counties that are both generally good and specifically strong for healthcare + bachelor's degree holders\n",
    "\n",
    "    The 70/30 weighting (70% base prediction, 30% career/education factors) maintains balance so you don't completely disregard general livability while still emphasizing your specific interests.\n",
    "        \n",
    "    '''\n",
    "\n",
    "    # Focus on the selected career/industry for boosting\n",
    "    if selected_career in career_field_features:\n",
    "        career_column = career_field_features[selected_career]\n",
    "        if career_column in clean_filtered_tennessee_gdf.columns:\n",
    "            #Create ratio to see if percentage is above or below average\n",
    "            career_boost = clean_filtered_tennessee_gdf[career_column] / clean_filtered_tennessee_gdf[career_column].mean()\n",
    "            #The formula gives 70% weight to the original score and 30% to the career-specific boost\n",
    "            prediction_df['predicted_score'] *= (0.7 + 0.3 * career_boost.fillna(0))\n",
    "\n",
    "    # If education level is selected, you could also adjust the score for education-specific features, if desired\n",
    "    if selected_education_level in education_levels:\n",
    "        education_column = education_levels[selected_education_level]\n",
    "        if education_column in clean_filtered_tennessee_gdf.columns:\n",
    "            #Create ratio to see if percentage is above or below average\n",
    "            education_boost = clean_filtered_tennessee_gdf[education_column] / clean_filtered_tennessee_gdf[education_column].mean()\n",
    "            #The formula gives 70% weight to the original score and 30% to the career-specific boost\n",
    "            prediction_df['predicted_score'] *= (0.7 + 0.3 * education_boost.fillna(0))\n",
    "\n",
    "\n",
    "    # Normalize to 0-100 range\n",
    "    min_score = prediction_df['predicted_score'].min()\n",
    "    max_score = prediction_df['predicted_score'].max()\n",
    "    prediction_df['predicted_score'] = 100 * (prediction_df['predicted_score'] - min_score) / (max_score - min_score)\n",
    "\n",
    "    # Return the sorted dataframe based on the predicted score, highest first\n",
    "    return prediction_df.sort_values(by='predicted_score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b3c3f",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "With the model created, let's add interactivity to it and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Random Forest model...\")\n",
    "model, feature_names = train_career_hotspot_rf_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\nFeature importances:\")\n",
    "for idx in sorted_indices:\n",
    "    print(f\"{idx}: {feature_names[idx] if idx < len(feature_names) else 'Career Feature'} - {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef96953",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh([feature_names[idx] if idx < len(feature_names) else f\"Career Feature {idx}\" for idx in sorted_indices],\n",
    "         importances[sorted_indices])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Feature Importances from Random Forest\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f301c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAvailable career fields:\")\n",
    "for i, career in enumerate(career_field_features.keys(), 1):\n",
    "    print(f\"{i}. {career.title()}\")\n",
    "career_idx = int(input(\"\\nEnter the number for your desired career field: \")) - 1\n",
    "career_field = list(career_field_features.keys())[career_idx]\n",
    "career_value = career_field_features[career_field]\n",
    "\n",
    "\n",
    "# Display available education levels\n",
    "print(\"\\nAvailable education levels:\")\n",
    "for i, edu_level in enumerate(education_levels.keys(), 1):\n",
    "    print(f\"{i}. {edu_level.replace('_', ' ').title()}\")\n",
    "edu_idx = int(input(\"\\nEnter the number for your education level: \")) - 1\n",
    "education_level = list(education_levels.keys())[edu_idx]    \n",
    "education_value = education_levels[education_level]\n",
    "\n",
    "\n",
    "predictions = predict_with_model_for_industry(model, feature_names, career_value, education_value)\n",
    "\n",
    "predictions = predictions.sort_values('predicted_score', ascending=False)\n",
    "print(f\"\\nTop 10 recommended counties for {career_field.title()} career (ML predictions):\")\n",
    "top_counties = predictions.head(10)\n",
    "display(top_counties)\n",
    "\n",
    "\n",
    "\n",
    "tn_plot_gdf = pd.merge(\n",
    "    clean_filtered_tennessee_gdf,\n",
    "    predictions[['GEOID', 'predicted_score']],\n",
    "    on=['GEOID'],\n",
    "    how='left'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Create the figure with hover data\n",
    "fig = px.choropleth_map(\n",
    "    tn_plot_gdf,\n",
    "    geojson=tn_plot_gdf.geometry.__geo_interface__,\n",
    "    locations=tn_plot_gdf.index,\n",
    "    color='predicted_score',\n",
    "    color_continuous_scale='Blues',\n",
    "    hover_data=['GEOID', 'COUNTY', \n",
    "                'ACS_TOT_CIVIL_EMPLOY_POP', 'ACS_MEDIAN_HH_INC', 'ACS_PCT_POSTHS_ED',\n",
    "                'predicted_score'],\n",
    "    opacity=0.8,\n",
    "    center={\"lat\": 35.86, \"lon\": -86.36},\n",
    "    zoom=7,\n",
    "    map_style=\"carto-positron\",\n",
    "    labels={'predicted_score': 'Predicted Score'}\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Predicted Scores by Census Tract',\n",
    "    margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 0},\n",
    "    height=700,\n",
    "    coloraxis_colorbar=dict(\n",
    "        title=\"Score\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update hover template\n",
    "fig.update_traces(\n",
    "    hovertemplate='<b>GEOID: %{customdata[0]}</b><br>' +\n",
    "                  'County: %{customdata[1]}<br>' +\n",
    "                  'Total Employed Population: %{customdata[2]}<br>' +\n",
    "                  'Household Income: $%{customdata[3]}<br>' +\n",
    "                  'Postsecondary Education: %{customdata[4]}%<br>' +\n",
    "                  'Predicted Score: %{customdata[5]:.2f}<br>'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "tn_plot_gdf.plot(\n",
    "    column='predicted_score',\n",
    "    cmap='viridis', \n",
    "    legend=True,\n",
    "    legend_kwds={'label': \"Suitability Score\", 'orientation': \"horizontal\"},\n",
    "    ax=ax\n",
    ")\n",
    "plt.title(f'Career Suitability Scores for {career_field.title()} with {education_level.replace(\"_\", \" \").title()}', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('career_suitability_map.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
